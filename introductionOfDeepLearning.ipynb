{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP -  Introduction to Pytorch\n",
    "\n",
    "\n",
    "## Tensors, Back-propagation, Hand-written digit recognition,  Language modeling\n",
    "*************************************************************\n",
    "\n",
    "**Recommended Reading:**\n",
    "\n",
    "http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0000e+00 -3.6893e+19\n",
      " 1.7398e-08  2.5250e-29\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "-9.223372036854776e+18\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.FloatTensor(2, 2)\n",
    "print(a)\n",
    "print(a.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### are Numpy arrays inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.00000000e+00   0.00000000e+00]\n",
      " [  1.73983832e-08   2.52496909e-29]]\n"
     ]
    }
   ],
   "source": [
    "a = a.numpy()\n",
    "a [0, 1] = 0\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.ones(2, 2))\n",
    "print(x)\n",
    "print(x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=torch.Tensor(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.LongTensor with no dimension]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=Variable(torch.ones(1,1))\n",
    "z=Variable(torch.ones(1,1))\n",
    "y==z\n",
    "t= Variable(torch.zeros(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1\n",
       "[torch.ByteTensor of size 1x1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y==z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1\n",
       "[torch.ByteTensor of size 1x1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y-z==t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Criterion(x,y):\n",
    "    return 1 if x-y is t else 0\n",
    "Criterion(y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### are Tensors inside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with automatic differentiation !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-propagation: example\n",
    "\n",
    "On calcule les gradients $\\frac{\\partial L(y - h(x))}{\\partial w}$ et $\\frac{\\partial L(y - h(x))}{\\partial b}$, avec:\n",
    " - $h(x) = \\sigma(w*x + b)$\n",
    " - $\\sigma$ est la fonction logistique (sigmoid)\n",
    " - $L(y, \\hat{y}) = (y - \\hat{y})^2$ (erreur quadratique)\n",
    " - $y = 0.2$\n",
    " - $x = 1.5$\n",
    " - $b = -2$\n",
    " - $w = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9241418242454529\n",
      "0.1522950828075409\n",
      "0.101530060172081\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([1.5]))\n",
    "y = Variable(torch.Tensor([0.2]))\n",
    "b = Variable(torch.Tensor([-2.0]), requires_grad=True)\n",
    "w = Variable(torch.Tensor([3.0]), requires_grad=True)\n",
    "h = torch.sigmoid(w * x + b)\n",
    "error = (y - h)**2\n",
    "error.backward()\n",
    "\n",
    "print(h.data[0])\n",
    "print(w.grad.data[0])\n",
    "print(b.grad.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.5244\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On minimize $L(y - h(x))$ pas à pas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LongTensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-8784508f0500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#b = Variable(torch.Tensor([-2.0]), requires_grad=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#w = Variable(torch.Tensor([3.0]), requires_grad=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LongTensor' is not defined"
     ]
    }
   ],
   "source": [
    "#x = Variable(torch.Tensor([1.5]))\n",
    "#y = Variable(torch.Tensor([0.2]))\n",
    "x=1\n",
    "y=1\n",
    "#b = Variable(torch.Tensor([-2.0]), requires_grad=True)\n",
    "#w = Variable(torch.Tensor([3.0]), requires_grad=True)\n",
    "b = Variable(LongTensor)\n",
    "alpha = 0.2\n",
    "for i in range(10):\n",
    "    h = torch.sigmoid(w * x + b)\n",
    "    error = (y - h)**2\n",
    "    error.backward()\n",
    "    \n",
    "    w.data.sub_(alpha * w.grad.data)\n",
    "    w.grad.data.zero_()  # must reset to 0 before next pass\n",
    "    \n",
    "    b.data.sub_(alpha * b.grad.data)\n",
    "    b.grad.data.zero_()  # must reset to 0 before next pass\n",
    "    \n",
    "    print('Epoch {} h={:.02f} w={:.02f} b={:.02f} error={:.02f}'.format(i+1, h.data[0], w.data[0], b.data[0], error.data[0]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[1,0,3]\n",
    "x = Variable(torch.Tensor(t), requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    loss1 = sum(x-1)\n",
    "    loss1.backward()\n",
    "    x.data -= 0.05*x.grad.data\n",
    "    print('epoch {} loss1 {:.1f}'.format(i+1, loss1.data[0]))\n",
    "    x.grad.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Variable(torch.LongTensor(t), requires_grad=True)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5*y.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    loss2 = sum(y-1)\n",
    "    loss2.backward()\n",
    "    y.data -= y.grad.data\n",
    "    print('epoch {} loss2 {:.1f} y1 {} y2 {} y3 {}'.format(i+1, loss2.data[0], y.data[0], y.data[1], y.data[2]))\n",
    "    y.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayez différentes valeurs pour le paramètre alpha: 0.01, 0.1, 1, 10, 100. Que constatez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor([1.5]))\n",
    "y = Variable(torch.Tensor([0.2]))\n",
    "b = Variable(torch.Tensor([-2.0]), requires_grad=True)\n",
    "w = Variable(torch.Tensor([3.0]), requires_grad=True)\n",
    "\n",
    "alpha = 0.01\n",
    "for i in range(100):\n",
    "    h = torch.sigmoid(w * x + b)\n",
    "    error = (y - h)**2\n",
    "    error.backward()\n",
    "    \n",
    "    w.data.sub_(alpha * w.grad.data)\n",
    "    w.grad.data.zero_()  # must reset to 0 before next pass\n",
    "    \n",
    "    b.data.sub_(alpha * b.grad.data)\n",
    "    b.grad.data.zero_()  # must reset to 0 before next pass\n",
    "    \n",
    "    print('Epoch {} h={:.05f} w={:.05f} b={:.05f} error={:.05f}'.format(i+1, h.data[0], w.data[0], b.data[0], error.data[0]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor([1.5]))\n",
    "y = Variable(torch.Tensor([0.2]))\n",
    "b = Variable(torch.Tensor([-2.0]), requires_grad=True)\n",
    "w = Variable(torch.Tensor([3.0]), requires_grad=True)\n",
    "\n",
    "alpha = 0.1\n",
    "for i in range(100):\n",
    "    h = torch.sigmoid(w * x + b)\n",
    "    error = (y - h)**2\n",
    "    error.backward()\n",
    "    \n",
    "    w.data.sub_(alpha * w.grad.data)\n",
    "    w.grad.data.zero_()  # must reset to 0 before next pass\n",
    "    \n",
    "    b.data.sub_(alpha * b.grad.data)\n",
    "    b.grad.data.zero_()  # must reset to 0 before next pass\n",
    "    \n",
    "    print('Epoch {} h={:.05f} w={:.05f} b={:.05f} error={:.05f}'.format(i+1, h.data[0], w.data[0], b.data[0], error.data[0]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quand le paramètre alpha est trop grand, les suites w,b ne convergent pas, ou en ce cas, converge à un minimum local.\n",
    "Quand le paramètre est trop petit, la convergence est très lentement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A vous de jouer !\n",
    "\n",
    "Voici un jeu de données synthetique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "X = np.concatenate((\n",
    "    np.random.random(\n",
    "        size=(5000, 2)) - 0.5,\n",
    "    np.random.multivariate_normal(\n",
    "        size=(2500, ),\n",
    "        mean=(-.25, -.25),\n",
    "        cov=((0.005, 0), (0, 0.005))),\n",
    "    np.random.multivariate_normal(\n",
    "        size=(2500, ),\n",
    "        mean=(0.25, 0.25),\n",
    "        cov=((0.005, 0), (0, 0.005))),\n",
    "))\n",
    "#Donc, X_ est de dimension 10000x2????\n",
    "\n",
    "Y = np.concatenate((\n",
    "    np.zeros(shape=(5000, 1)),\n",
    "    np.ones(shape=(5000, 1)),\n",
    "))\n",
    "\n",
    "#Y_ de dimension 10000x1\n",
    "\n",
    "X = np.asarray(X, dtype='float32')#bien X_ thanh 1 mang so thuc,Y_ pareil\n",
    "Y = np.asarray(Y, dtype='float32')\n",
    "\n",
    "# shuffle data points\n",
    "perm = np.random.permutation(X.shape[0])\n",
    "X = X[perm]\n",
    "Y = Y[perm]\n",
    "\n",
    "# numpy arrays -> torch tensors\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "Y = torch.from_numpy(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée deux jeux séparés: train / test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# train / test split\n",
    "n_train = 100\n",
    "n_test = 100\n",
    "\n",
    "X_train = X[:n_train]\n",
    "Y_train = Y[:n_train]\n",
    "\n",
    "X_test = X[n_train:(n_train + n_test)]\n",
    "Y_test = Y[n_train:(n_train + n_test)]\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 8), squeeze=False)\n",
    "axes[0, 0].set_title('train')\n",
    "axes[0, 0].scatter(\n",
    "    X_train[:, 0].numpy(),\n",
    "    X_train[:, 1].numpy(),\n",
    "    c=Y_train[:, 0].numpy(),\n",
    "    cmap=cm_bright,\n",
    "    edgecolors='k')\n",
    "axes[0, 1].set_title('test')\n",
    "axes[0, 1].scatter(\n",
    "    X_test[:, 0].numpy(),\n",
    "    X_test[:, 1].numpy(),\n",
    "    c=Y_test[:, 0].numpy(),\n",
    "    cmap=cm_bright,\n",
    "    edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentez un réseau de neurones sans couche cachée, qui prends en entrée $\\mathbf{x} \\in \\mathbb{R}^2$ et produit une seule sortie $\\mathbf{y} \\in [0, 1]$ (sigmoid).\n",
    "\n",
    "Multiplication de matrics: torch.matmul\n",
    "\n",
    "Interdiction d'utiliser les modules haut niveau de pytorch ! (optim, nn etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "n_in = 2\n",
    "params = []\n",
    "\n",
    "n_out = 1\n",
    "W = Variable(torch.normal(torch.zeros(n_in, n_out), np.sqrt(2/(n_in + n_out))), requires_grad=True)\n",
    "b = Variable(torch.zeros(n_out), requires_grad=True)\n",
    "\n",
    "def forward(X):\n",
    "    H  = torch.sigmoid(torch.mm(X,W)  + b)\n",
    "    \n",
    "    return H #O day H kha nang la ham sigmoid theo X, nhung con thieu W va b. Chac dung o tren\n",
    "\n",
    "# Dung la ham sigmoid.\n",
    "\n",
    "def L(H, Y, eps=1e-08):\n",
    "    loss = sum(-Y * torch.log(H + eps) - (1 - Y) * torch.log(1 - H + eps))  # log-likelikood\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "n_epochs = 250\n",
    "epoch_n_batches = 100\n",
    "train_batch_size = 10\n",
    "alpha = 0.1\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.\n",
    "\n",
    "    for j in range(epoch_n_batches):\n",
    "\n",
    "        # Prepare next mini-batch\n",
    "        mb_idxs = torch.multinomial(torch.ones(n_train), train_batch_size, replacement=True)\n",
    "        #mb_idxs pour choisir aléatoirement train_batch_size éléments dans l'intervalle 0, n_train\n",
    "        #afin de choisir aléatoirement un petit échantillon pour mini path\n",
    "        X_mb = Variable(X_train[mb_idxs])\n",
    "        Y_mb = Variable(Y_train[mb_idxs])\n",
    "\n",
    "        # Forward pass\n",
    "        Y_prob_mb = forward(X_mb)\n",
    "        \n",
    "        loss = L(Y_prob_mb,Y_mb)\n",
    "        loss.backward()\n",
    "        W.data.sub_(alpha * W.grad.data)\n",
    "        W.grad.data.zero_()  # must reset to 0 before next pass\n",
    "    \n",
    "        b.data.sub_(alpha * b.grad.data)\n",
    "        b.grad.data.zero_()  # must reset to 0 before next pass\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "\n",
    "    train_loss /= epoch_n_batches\n",
    "\n",
    "    # Forward pass\n",
    "    Y_test_prob = forward(Variable(X_test)).data\n",
    "    test_loss = L(Y_test_prob, Y_test).mean()\n",
    "    \n",
    "    # Compute expected 0/1 error\n",
    "    Y_test_pred = (Y_test_prob > 0.5).type(torch.FloatTensor)\n",
    "    test_err = torch.abs(Y_test - Y_test_pred).mean()\n",
    "\n",
    "    print('Epoch {:03d}\\ttrain loss={:.06f}\\ttest loss={:.06f}\\t0/1 error={:.03f}'.format(\n",
    "        i + 1, train_loss, test_loss, test_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisez la distribution apprise par votre modèle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_size = (100, 100)\n",
    "x1, x2 = np.meshgrid(np.linspace(X_[:, 0].min()-0.1, X_[:, 0].max()+0.1, map_size[0]),\n",
    "                     np.linspace(X_[:, 1].min()-0.1, X_[:, 1].max()+0.1, map_size[1]))\n",
    "X_map = torch.from_numpy(np.asarray(np.c_[x1.ravel(), x2.ravel()], dtype='float32'))\n",
    "\n",
    "Y_prob_map = forward(Variable(X_map)).data\n",
    "\n",
    "Y_train_pred = (forward(Variable(X_train)).data > 0.5).type(torch.FloatTensor)\n",
    "train_err = torch.abs(Y_train - Y_train_pred).mean()\n",
    "\n",
    "Y_test_pred = (forward(Variable(X_test)).data > 0.5).type(torch.FloatTensor)\n",
    "test_err = torch.abs(Y_test - Y_test_pred).mean()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 8), squeeze=False)\n",
    "axes[0, 0].set_title('train accuracy={:.03f}'.format(1 - train_err))\n",
    "axes[0, 0].contourf(\n",
    "    x1,\n",
    "    x2,\n",
    "    Y_prob_map.numpy().reshape(map_size),\n",
    "    cmap=cm, vmin=0, vmax=1,\n",
    "    alpha=.8)\n",
    "axes[0, 0].scatter(\n",
    "    X_train[:, 0].numpy(),\n",
    "    X_train[:, 1].numpy(),\n",
    "    c=Y_train[:, 0].numpy(),\n",
    "    cmap=cm_bright,\n",
    "    edgecolors='k')\n",
    "axes[0, 1].set_title('test accuracy={:.03f}'.format(1 - test_err))\n",
    "axes[0, 1].contourf(\n",
    "    x1,\n",
    "    x2,\n",
    "    Y_prob_map.numpy().reshape(map_size),\n",
    "    cmap=cm, vmin=0, vmax=1,\n",
    "    alpha=.8)\n",
    "axes[0, 1].scatter(\n",
    "    X_test[:, 0].numpy(),\n",
    "    X_test[:, 1].numpy(),\n",
    "    c=Y_test[:, 0].numpy(),\n",
    "    cmap=cm_bright,\n",
    "    edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajoutez une couche cachée de taille 10 à votre réseau de neurones, avec la fonction d'activation relu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: améliorez votre réseau de neurones: plus de neurones cachés, deuxième couche cachée etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "n_in = 2\n",
    "n_c1=10\n",
    "params = []\n",
    "\n",
    "n_out = 1\n",
    "W1 = Variable(torch.normal(torch.zeros(n_in, n_c1), np.sqrt(2/(n_in + n_c1))), requires_grad=True)\n",
    "b1 = Variable(torch.zeros(n_c1), requires_grad=True)\n",
    "W2 = Variable(torch.normal(torch.zeros(n_c1, n_out), np.sqrt(2/(n_c1 + n_out))), requires_grad=True)\n",
    "b2 = Variable(torch.zeros(n_out), requires_grad=True)\n",
    "\n",
    "def forward(X):\n",
    "    H1 = torch.sigmoid( torch.mm(X, W1)+b1)\n",
    "    H2 = torch.sigmoid(torch.mm(H1, W2) + b2)\n",
    "    \n",
    "    return H2 \n",
    "\n",
    "def L(H, Y, eps=1e-08):\n",
    "    loss = sum(-Y * torch.log(H + eps) - (1 - Y) * torch.log(1 - H + eps))  # log-likelikood\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "n_epochs = 250\n",
    "epoch_n_batches = 100\n",
    "train_batch_size = 10\n",
    "alpha = 0.1\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.\n",
    "\n",
    "    for j in range(epoch_n_batches):\n",
    "\n",
    "        # Prepare next mini-batch\n",
    "        mb_idxs = torch.multinomial(torch.ones(n_train), train_batch_size, replacement=True)\n",
    "        X_mb = Variable(X_train[mb_idxs])\n",
    "        Y_mb = Variable(Y_train[mb_idxs])\n",
    "\n",
    "        # Forward pass\n",
    "        Y_prob_mb = forward(X_mb)\n",
    "        \n",
    "        loss = L(Y_prob_mb,Y_mb)\n",
    "        # loss la ham sum cua y_ilogf(x_i)+(1-y_i)log(1-f(x_i)) vay no la sum cua ham L(H,y_i)\n",
    "\n",
    "        # Backward pass# \n",
    "        loss.backward()\n",
    "        # Parameter update (gradient descent)\n",
    "        W1.data.sub_(alpha * W1.grad.data)\n",
    "        W1.grad.data.zero_()  # must reset to 0 before next pass\n",
    "        W2.data.sub_(alpha * W2.grad.data)\n",
    "        W2.grad.data.zero_()  # must reset to 0 before next pass\n",
    "    \n",
    "        b1.data.sub_(alpha * b1.grad.data)\n",
    "        b1.grad.data.zero_()  # must reset to 0 before next pass\n",
    "\n",
    "        b2.data.sub_(alpha * b2.grad.data)\n",
    "        b2.grad.data.zero_()  # must reset to 0 before next pass\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "\n",
    "    train_loss /= epoch_n_batches\n",
    "\n",
    "    # Forward pass\n",
    "    Y_test_prob = forward(Variable(X_test)).data\n",
    "    test_loss = L(Y_test_prob, Y_test).mean()\n",
    "    \n",
    "    # Compute expected 0/1 error\n",
    "    Y_test_pred = (Y_test_prob > 0.5).type(torch.FloatTensor)\n",
    "    test_err = torch.abs(Y_test - Y_test_pred).mean()\n",
    "\n",
    "    print('Epoch {:03d}\\ttrain loss={:.06f}\\ttest loss={:.06f}\\t0/1 error={:.03f}'.format(\n",
    "        i + 1, train_loss, test_loss, test_err))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "n_in = 2\n",
    "n_c1=10\n",
    "n_c2=10\n",
    "params = []\n",
    "\n",
    "n_out = 1\n",
    "W1 = Variable(torch.normal(torch.zeros(n_in, n_c1), np.sqrt(2/(n_in + n_c1))), requires_grad=True)\n",
    "b1 = Variable(torch.zeros(n_c1), requires_grad=True)\n",
    "W2 = Variable(torch.normal(torch.zeros(n_c1, n_c2), np.sqrt(2/(n_c1 + n_c2))), requires_grad=True)\n",
    "b2 = Variable(torch.zeros(n_c2), requires_grad=True)\n",
    "W3 = Variable(torch.normal(torch.zeros(n_c2, n_out), np.sqrt(2/(n_c2 + n_out))), requires_grad=True)\n",
    "b3 = Variable(torch.zeros(n_out), requires_grad=True)\n",
    "\n",
    "\n",
    "def forward(X):\n",
    "    H1 = torch.sigmoid( torch.mm(X, W1)+b1)\n",
    "    H2 = torch.sigmoid(torch.mm(H1, W2) + b2)\n",
    "    H3 = torch.sigmoid(torch.mm(H2, W3) + b3)\n",
    "    return H3 \n",
    "\n",
    "def L(H, Y, eps=1e-08):\n",
    "    loss = sum (-Y * torch.log(H + eps) - (1 - Y) * torch.log(1 - H + eps))  # log-likelikood\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "n_epochs = 250\n",
    "epoch_n_batches = 100\n",
    "train_batch_size = 10\n",
    "alpha = 0.1\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.\n",
    "\n",
    "    for j in range(epoch_n_batches):\n",
    "\n",
    "        # Prepare next mini-batch\n",
    "        mb_idxs = torch.multinomial(torch.ones(n_train), train_batch_size, replacement=True)\n",
    "        X_mb = Variable(X_train[mb_idxs])\n",
    "        Y_mb = Variable(Y_train[mb_idxs])\n",
    "\n",
    "        # Forward pass\n",
    "        Y_prob_mb = forward(X_mb)\n",
    "        \n",
    "        loss = L(Y_prob_mb,Y_mb)\n",
    "        loss.backward()\n",
    "        W1.data.sub_(alpha * W1.grad.data)\n",
    "        W1.grad.data.zero_()  # must reset to 0 before next pass\n",
    "        W2.data.sub_(alpha * W2.grad.data)\n",
    "        W2.grad.data.zero_()  # must reset to 0 before next pass\n",
    "        W3.data.sub_(alpha * W3.grad.data)\n",
    "        W3.grad.data.zero_()  # must reset to 0 before next pass\n",
    "    \n",
    "        b1.data.sub_(alpha * b1.grad.data)\n",
    "        b1.grad.data.zero_()  # must reset to 0 before next pass\n",
    "\n",
    "        b2.data.sub_(alpha * b2.grad.data)\n",
    "        b2.grad.data.zero_()  # must reset to 0 before next pass\n",
    "        b3.data.sub_(alpha * b3.grad.data)\n",
    "        b3.grad.data.zero_()  # must reset to 0 before next pass\n",
    "        train_loss += loss.data[0]\n",
    "\n",
    "    train_loss /= epoch_n_batches\n",
    "\n",
    "    # Forward pass\n",
    "    Y_test_prob = forward(Variable(X_test)).data\n",
    "    test_loss = L(Y_test_prob, Y_test).mean()\n",
    "    \n",
    "    # Compute expected 0/1 error\n",
    "    Y_test_pred = (Y_test_prob > 0.5).type(torch.FloatTensor)\n",
    "    test_err = torch.abs(Y_test - Y_test_pred).mean()\n",
    "\n",
    "    print('Epoch {:03d}\\ttrain loss={:.06f}\\ttest loss={:.06f}\\t0/1 error={:.03f}'.format(\n",
    "        i + 1, train_loss, test_loss, test_err))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MNIST\n",
    "\n",
    "Vous allez maintenant utiliser les fonctions haut niveau de pytorch: torch.nn, torch.optim etc.\n",
    "\n",
    "On commence par charger les données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "mnist_folder = '!/MNIST_data/'\n",
    "\n",
    "train_data = datasets.MNIST(root=mnist_folder, download=True, transform=transforms.ToTensor(), train=True)\n",
    "test_data = datasets.MNIST(root=mnist_folder, download=True, transform=transforms.ToTensor(), train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre d'images dans le train set:\", len(train_data))\n",
    "print(\"Nombre d'images dans le train set:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afficher quelques images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(8, 4), squeeze=False)\n",
    "seed = 123 # changer la seed pour afficher d'autres images\n",
    "\n",
    "for i in range(10):\n",
    "    r = i // 5\n",
    "    c = i % 5\n",
    "    np.random.seed(seed+i)\n",
    "    idx = np.random.choice(len(train_data), 1)[0]\n",
    "    x = train_data[idx][0].numpy()\n",
    "    y = train_data[idx][1]\n",
    "    axes[r, c].imshow(x[0, :, :])\n",
    "    axes[r, c].set_title('y={}'.format(y))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Training settings\n",
    "seed = 1337\n",
    "lr = 0.2\n",
    "epochs = 25\n",
    "train_batch_size = 25\n",
    "test_batch_size = 100\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, 28*28)  # flatten images\n",
    "        X = self.fc1(X)\n",
    "        return F.log_softmax(X)\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "def train(epoch):\n",
    "\n",
    "    exp_loss = 0.\n",
    "    n_processed = 0\n",
    "\n",
    "    model.train()\n",
    "    for X, Y in train_loader:\n",
    "        X, Y = Variable(X), Variable(Y)\n",
    "        \n",
    "        # Forward pass\n",
    "        Y_pred_prob = model(X)\n",
    "\n",
    "        # Compute expected loss\n",
    "        loss = F.nll_loss(Y_pred_prob, Y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Parameter update (gradient descent)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # exp_loss.data[0] += loss * X.data.shape[0]\n",
    "        exp_loss += loss * X.data.shape[0]\n",
    "        n_processed += X.data.shape[0]\n",
    "    \n",
    "    exp_loss /= n_processed\n",
    "    return exp_loss\n",
    "\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    test_error = 0\n",
    "    model.eval()\n",
    "    for X, Y in test_loader:\n",
    "        X, Y = Variable(X, volatile=True), Variable(Y)\n",
    "        \n",
    "        # Forward pass\n",
    "        Y_pred_prob = model(X)\n",
    "        \n",
    "        # Compute the expected negative log-likelihood\n",
    "        test_loss += F.nll_loss(Y_pred_prob, Y, size_average=False).data[0]\n",
    "        \n",
    "        # Get the mode of p(y|x) (most probable digit)\n",
    "        Y_pred = Y_pred_prob.data.max(1, keepdim=True)[1]\n",
    "        \n",
    "        # Compute the expected 0/1 error\n",
    "        test_error += (1 - Y_pred.eq(Y.data.view_as(Y_pred))).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_error /= len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, test_error\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    print('Epoch {}...'.format(epoch))\n",
    "    \n",
    "    train_loss = train(epoch)\n",
    "    print('Train negative log-likelihood: {:.6f}'.format(train_loss.data[0]))\n",
    "    \n",
    "    test_loss, test_error = test()\n",
    "    print('Test  negative log-likelihood: {:.6f} 0/1 error: {:.6f}'.format(test_loss, test_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jouez avec les paramètres suivants:\n",
    "- lr\n",
    "- epochs\n",
    "- train_batch_size\n",
    "\n",
    "Quelle est la meilleure performance (0/1 error) que vous arrivez à atteindre?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le meilleur score que j'obtient est 0.0751 avec lr=0.2, epochs=25, train_batch_size=50\n",
    "J'ai monté epochs jusqu'à 100 mais le résultat ne s'améliore pas. Il faut donc changer la méthode pour espérér un meilleur résultat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Améliorez votre modèle\n",
    "\n",
    "Améliorez votre modèle afin de diminuer l'erreur sur le jeu de test. Essayez différentes architectures / hyperparamètres et à chaque fois reportez vos résultats.\n",
    "\n",
    "Objectif: passer sous les 1% d'erreur: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Pistes à essayer:\n",
    "- plus de couches cachées\n",
    "- convolutions + max pooling\n",
    "- dropout\n",
    "- couches résiduelles\n",
    "\n",
    "Astuces:\n",
    "- changer l'algorithme de descente de gradient (Adam)\n",
    "- implémentez une stratégie d'early stopping: $n$ epochs sans amélioration -> arret\n",
    "- implémentez une stratégie de diminution du learning rate: $n/2$ epochs sans amélioration -> $\\alpha = \\alpha / 2$ (torch.optim.lr_scheduler.ReduceLROnPlateau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "t_s=time.time()\n",
    "# Training settings\n",
    "seed = 1337\n",
    "lr = 1\n",
    "epochs = 5\n",
    "train_batch_size = 25\n",
    "test_batch_size = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 120)# On utilise ici un NN avec 3 couche, donc, 2 couches cachées de dimension \n",
    "        self.fc2 = nn.Linear(120, 80)# correspondance de (28*28, 120) et (120,84)\n",
    "        self.fc3 = nn.Linear(80, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, 28*28)\n",
    "        X = self.fc1(X)\n",
    "        X= F.log_softmax(X)\n",
    "        X = self.fc2(X)\n",
    "        X = F.log_softmax(X)\n",
    "        X = self.fc3(X)\n",
    "        return F.log_softmax(X)\n",
    "        \n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "def train(epoch):\n",
    "\n",
    "    exp_loss = 0.\n",
    "    n_processed = 0\n",
    "\n",
    "    model.train()\n",
    "    for X, Y in train_loader:\n",
    "        X, Y = Variable(X), Variable(Y)\n",
    "        \n",
    "        # Forward pass\n",
    "        Y_pred_prob = model(X)\n",
    "\n",
    "        # Compute expected loss\n",
    "        loss = F.nll_loss(Y_pred_prob, Y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Parameter update (gradient descent)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # exp_loss.data[0] += loss * X.data.shape[0]\n",
    "        exp_loss += loss * X.data.shape[0]\n",
    "        n_processed += X.data.shape[0]\n",
    "    \n",
    "    exp_loss /= n_processed\n",
    "    return exp_loss\n",
    "\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    test_error = 0\n",
    "    \n",
    "    model.eval()\n",
    "    for X, Y in test_loader:\n",
    "        X, Y = Variable(X, volatile=True), Variable(Y)\n",
    "        \n",
    "        # Forward pass\n",
    "        Y_pred_prob = model(X)\n",
    "        \n",
    "        # Compute the expected negative log-likelihood\n",
    "        test_loss += F.nll_loss(Y_pred_prob, Y, size_average=False).data[0]\n",
    "        \n",
    "        # Get the mode of p(y|x) (most probable digit)\n",
    "        Y_pred = Y_pred_prob.data.max(1, keepdim=True)[1]\n",
    "        \n",
    "        # Compute the expected 0/1 error\n",
    "        test_error += (1 - Y_pred.eq(Y.data.view_as(Y_pred))).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_error /= len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, test_error\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    print('Epoch {}...'.format(epoch))\n",
    "    \n",
    "    train_loss = train(epoch)\n",
    "    print('Train negative log-likelihood: {:.6f}'.format(train_loss.data[0]))\n",
    "    \n",
    "    test_loss, test_error = test()\n",
    "    print('Test  negative log-likelihood: {:.6f} 0/1 error: {:.6f}'.format(test_loss, test_error))\n",
    "\n",
    "t_end=time.time()\n",
    "print(\"Time\", t_end-t_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "t_s=time.time()\n",
    "# Training settings\n",
    "seed = 1337\n",
    "lr = 0.1\n",
    "epochs = 10\n",
    "train_batch_size = 25\n",
    "test_batch_size = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "def train(epoch):\n",
    "\n",
    "    exp_loss = 0.\n",
    "    n_processed = 0\n",
    "\n",
    "    model.train()\n",
    "    for X, Y in train_loader:\n",
    "        X, Y = Variable(X), Variable(Y)\n",
    "        \n",
    "        # Forward pass\n",
    "        Y_pred_prob = model(X)\n",
    "\n",
    "        # Compute expected loss\n",
    "        loss = F.nll_loss(Y_pred_prob, Y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Parameter update (gradient descent)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # exp_loss.data[0] += loss * X.data.shape[0]\n",
    "        exp_loss += loss * X.data.shape[0]\n",
    "        n_processed += X.data.shape[0]\n",
    "    \n",
    "    exp_loss /= n_processed\n",
    "    return exp_loss\n",
    "\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    test_error = 0\n",
    "    \n",
    "    model.eval()\n",
    "    for X, Y in test_loader:\n",
    "        X, Y = Variable(X, volatile=True), Variable(Y)\n",
    "        \n",
    "        # Forward pass\n",
    "        Y_pred_prob = model(X)\n",
    "        \n",
    "        # Compute the expected negative log-likelihood\n",
    "        test_loss += F.nll_loss(Y_pred_prob, Y, size_average=False).data[0]\n",
    "        \n",
    "        # Get the mode of p(y|x) (most probable digit)\n",
    "        Y_pred = Y_pred_prob.data.max(1, keepdim=True)[1]\n",
    "        \n",
    "        # Compute the expected 0/1 error\n",
    "        test_error += (1 - Y_pred.eq(Y.data.view_as(Y_pred))).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_error /= len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, test_error\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    print('Epoch {}...'.format(epoch))\n",
    "    \n",
    "    train_loss = train(epoch)\n",
    "    print('Train negative log-likelihood: {:.6f}'.format(train_loss.data[0]))\n",
    "    \n",
    "    test_loss, test_error = test()\n",
    "    print('Test  negative log-likelihood: {:.6f} 0/1 error: {:.6f}'.format(test_loss, test_error))\n",
    "\n",
    "t_end=time.time()\n",
    "print(\"Time\", t_end-t_s)#cette méthode tourne bien, nous avons donc une erreur de moins de 2% avec 10 epochs, \n",
    "#si on utilise 25 epochs, je pense qu'on peut arriver à un résultat de 1% un peu près."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dans le résultat précédent on peut espérer à avoir une erreur de 1% environs en augementant la taille de epoch(50, par exemple)  et la taille de training_batch_size(=50, par exemple. Là, je suis sur mon PC personnel et cela tournera mal avec les paramètres unpeu plus élévé)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## NLP: N-Gram Language Modeling\n",
    "\n",
    "Modèle NGram: sachant les mots $n$ précédents on veut prédire le prochain mot:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1})\n",
    "\\end{equation*}\n",
    "\n",
    "Où $w_i$ est le ième mot d'une phrase.\n",
    "\n",
    "Veuillez considérer de modèle de prédiction de trigram suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "t_s=time.time()\n",
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        context_idxs = [word_to_ix[w] for w in context]\n",
    "        context_var = Variable(torch.LongTensor(context_idxs))\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_var)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a variable)\n",
    "        loss = loss_function(log_probs, Variable(\n",
    "            torch.LongTensor([word_to_ix[target]])))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilisez vos propres valeurs pour context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ['to', 'be']\n",
    "context_idxs = [word_to_ix[w] for w in context]\n",
    "context_var = Variable(torch.LongTensor(context_idxs))\n",
    "log_probs = model(context_var)\n",
    "print(context, list(vocab)[torch.max(log_probs, 1)[1].data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ['How', 'much']\n",
    "context_idxs = [word_to_ix[w] for w in context]\n",
    "context_var = Variable(torch.LongTensor(context_idxs))\n",
    "log_probs = model(context_var)\n",
    "print(context, list(vocab)[torch.max(log_probs, 1)[1].data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ['This', 'child']\n",
    "context_idxs = [word_to_ix[w] for w in context]\n",
    "context_var = Variable(torch.LongTensor(context_idxs))\n",
    "log_probs = model(context_var)\n",
    "print(context, list(vocab)[torch.max(log_probs, 1)[1].data[0]])# il peut trouver la meilleure solution \n",
    "# meme quand ces 2 mots là ne sont pas consécutifs dans le texte original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW: Continuous Bag-of-Words\n",
    "CBOW = Prédire un mot sachant les mots d'avant et les mots d'après.\n",
    "\n",
    "Exercice : en utilisant l'exemple précédent, codez un modèle CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor)\n",
    "\n",
    "CONTEXT_SIZE = 4  # 2 words to the left, 2 to the right\n",
    "EMBEDDING_DIM = 10\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])\n",
    "\n",
    "#class CBOW(nn.Module):\n",
    "\n",
    " #   def __init__(self):\n",
    " #       pass\n",
    "\n",
    "  #  def forward(self, inputs):\n",
    "  #      pass\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range((300):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in data:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        context_var = make_context_vector(context, word_to_ix)\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_var)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a variable)\n",
    "        loss = loss_function(log_probs, Variable(\n",
    "            torch.LongTensor([word_to_ix[target]])))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!\n",
    "#là, nous avons une erreur très très petite..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ['processes', 'manipulate', 'abstract', 'things']# We expect to see 'other'\n",
    "context_var = make_context_vector(context, word_to_ix)\n",
    "log_probs = model(context_var)\n",
    "print(context, list(vocab)[torch.max(log_probs, 1)[1].data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ['People', 'create',  'to', 'direct']# We expect to see 'programs'\n",
    "context_var = make_context_vector(context, word_to_ix)\n",
    "log_probs = model(context_var)\n",
    "print(context, list(vocab)[torch.max(log_probs, 1)[1].data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ['Computational', 'processes',  'beings', 'that']# there are 'are' and 'abstract' between these words.\n",
    "context_var = make_context_vector(context, word_to_ix)\n",
    "log_probs = model(context_var)\n",
    "print(context, list(vocab)[torch.max(log_probs, 1)[1].data[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
